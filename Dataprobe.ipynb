{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"RD_LIB_CONFIG_PATH\"] = \"../../Configuration\"\n",
    "\n",
    "import refinitiv.data as rd\n",
    "from refinitiv.data.content import symbol_conversion\n",
    "\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import mysql.connector as mysql\n",
    "\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "import pandas_market_calendars as mcal\n",
    "\n",
    "\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# My library\n",
    "\n",
    "import utils.fidatabase as fi\n",
    "\n",
    "#import utils.variant_callback as vc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.open_session()\n",
    "db = fi.ConnectToDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined twice: HET.N^A08\n",
      "Deleting added ref HET.N^A08\n",
      "Joined twice: ROK.N\n",
      "Deleting added ref ROK.N\n",
      "Joined twice: MII.N^F99\n",
      "Deleting added ref MII.N^F99\n",
      "Joined twice: FJ.N^K00\n",
      "Deleting added ref FJ.N^K00\n",
      "Joined twice: RX.N^C10\n",
      "Deleting added ref RX.N^C10\n",
      "Joined twice: MWW.N^K16\n",
      "Deleting added ref MWW.N^K16\n",
      "Joined twice: PEG.N\n",
      "Deleting added ref PEG.N\n",
      "Joined twice: PARA.OQ\n",
      "Deleting added ref PARA.OQ\n",
      "Joined twice: JCI.N\n",
      "Deleting added ref JCI.N\n",
      "Not found to remove!:  FTI.N\n",
      "['CPB.N', 'COST.OQ', 'AXP.N', 'PCAR.OQ', 'CLX.N', 'AAPL.OQ', 'HON.OQ', 'NWL.OQ', 'BMY.N', 'T.N', 'AIG.N', 'TXN.OQ', 'EIX.N', 'SWK.N', 'GLW.N', 'IBM.N', 'MS.N', 'AMGN.OQ', 'UNP.N', 'GE.N', 'BA.N', 'EXC.OQ', 'PH.N', 'NSC.N', 'MCD.N', 'K.N', 'ITW.N', 'PG.N', 'D.N', 'HPQ.N', 'XOM.N', 'BKR.OQ', 'BALL.N', 'ROK.N', 'APD.N', 'CSX.OQ', 'LUV.N', 'LOW.N', 'WMB.N', 'SLB.N', 'CL.N', 'HES.N', 'COP.N', 'MSFT.OQ', 'IP.N', 'ETN.N', 'WFC.N', 'MMM.N', 'TRV.N', 'ECL.N', 'LLY.N', 'CAT.N', 'TAP.N', 'JPM.N', 'BDX.N', 'AVY.N', 'WMT.N', 'GL.N', 'MU.OQ', 'GD.N', 'XEL.OQ', 'DIS.N', 'FDX.N', 'DE.N', 'BFb.N', 'MRO.N', 'ADP.OQ', 'LNC.N', 'HWM.N', 'SYY.N', 'WY.N', 'NEE.N', 'CI.N', 'SO.N', 'KMB.N', 'GIS.N', 'INTC.OQ', 'MSI.N', 'DTE.N', 'UNH.N', 'ETR.N', 'CSCO.OQ', 'HAL.N', 'IFF.N', 'PKI.N', 'BBWI.N', 'HSY.N', 'KR.N', 'MO.N', 'PFE.N', 'MMC.N', 'CMI.N', 'WBA.OQ', 'NUE.N', 'NEM.N', 'WHR.N', 'F.N', 'TGT.N', 'PPG.N', 'NKE.N', 'KO.N', 'PEP.OQ', 'VFC.N', 'NOC.N', 'HD.N', 'DUK.N', 'VZ.N', 'ADM.N', 'SNA.N', 'JNJ.N', 'PNC.N', 'KEY.N', 'IPG.N', 'OXY.N', 'CAG.N', 'ADSK.OQ', 'MDT.N', 'EMN.N', 'GWW.N', 'EMR.N', 'ED.N', 'ORCL.N', 'DOV.N', 'CVX.N', 'MAS.N', 'TJX.N', 'SHW.N', 'AEP.OQ', 'BAX.N', 'RTX.N', 'LIN.N', 'MAR.OQ', 'GPC.N', 'CNP.N', 'MRK.N', 'PHM.N', 'PEG.N', 'BAC.N', 'HAS.OQ', 'SPGI.N', 'PARA.OQ', 'ABT.N', 'TXT.N', 'BSX.N', 'LMT.N', 'AMAT.OQ', 'BK.N', 'L.N', 'DRI.N', 'ALL.N', 'USB.N', 'FCX.N', 'PPL.N', 'HUM.N', 'CMA.N', 'HIG.N', 'FITB.OQ', 'AON.N', 'CVS.N', 'TMO.N', 'AZO.N', 'ADBE.OQ', 'CAH.N', 'SCHW.N', 'EFX.N', 'APA.OQ', 'PGR.N', 'HBAN.OQ', 'STT.N', 'KLAC.OQ', 'YUM.N', 'FE.N', 'TFC.N', 'CINF.OQ', 'OMC.N', 'AEE.N', 'NTRS.OQ', 'SEE.N', 'BEN.N', 'SRE.N', 'COF.N', 'WM.N', 'RF.N', 'PAYX.OQ', 'AES.N', 'C.N', 'DHR.N', 'CCL.N', 'MCK.N', 'CMS.N', 'AFL.N', 'NTAP.OQ', 'BBY.N', 'VMC.N', 'QCOM.OQ', 'PNW.N', 'ADI.OQ', 'TROW.OQ', 'A.N', 'SBUX.OQ', 'DVN.N', 'MCO.N', 'EOG.N', 'NI.N', 'RHI.N', 'INTU.OQ', 'MET.N', 'SYK.N', 'CTAS.OQ', 'FISV.OQ', 'ZION.OQ', 'ZBH.N', 'ABC.N', 'NVDA.OQ', 'EQR.N', 'WAT.N', 'SPG.N', 'GS.N', 'PFG.OQ', 'EA.OQ', 'UPS.N', 'PRU.N', 'EBAY.OQ', 'ELV.N', 'CMCSA.OQ', 'DGX.N', 'MKC.N', 'GEN.OQ', 'BIIB.OQ', 'MTB.N', 'VTRS.OQ', 'VLO.N', 'GILD.OQ', 'TPR.N', 'LH.N', 'DHI.N', 'STZ.N', 'TSN.N', 'PSA.N', 'AMP.N', 'LEN.N', 'AMZN.OQ', 'EL.N', 'VRSN.OQ', 'KIM.N', 'GOOGL.OQ', 'BXP.N', 'JNPR.N', 'CME.OQ', 'CBRE.N', 'FIS.N', 'CTSH.OQ', 'AVB.N', 'RL.N', 'CHRW.OQ', 'HST.OQ', 'MDLZ.OQ', 'AIZ.N', 'DFS.N', 'AKAM.OQ', 'MCHP.OQ', 'ICE.N', 'EXPE.OQ', 'EXPD.OQ', 'J.N', 'AMT.N', 'PM.N', 'PEAK.N', 'ISRG.OQ', 'CTRA.N', 'MA.N', 'DVA.N', 'IVZ.N', 'CF.N', 'FAST.OQ', 'CRM.N', 'LHX.N', 'PXD.N', 'APH.N', 'NDAQ.OQ', 'WEC.N', 'SJM.N', 'XRAY.OQ', 'WYNN.OQ', 'RSG.N', 'IRM.N', 'WELL.N', 'VTR.N', 'HRL.N', 'ES.N', 'ORLY.OQ', 'PWR.N', 'WDC.OQ', 'FMC.N', 'BKNG.OQ', 'V.N', 'ROST.OQ', 'ROP.N', 'NRG.N', 'BRKb.N', 'WBD.OQ', 'OKE.N', 'KMX.N', 'CB.N', 'JCI.N', 'TT.N', 'FFIV.OQ', 'NFLX.OQ', 'EW.N', 'BLK.N', 'CMG.N', 'PLD.N', 'MPC.N', 'ACN.N', 'MOS.N', 'TEL.N', 'XYL.N', 'DLTR.OQ', 'BWA.N', 'CCI.N', 'PSX.N', 'KMI.N', 'LRCX.OQ', 'MNST.OQ', 'STX.OQ', 'LYB.N', 'PNR.N', 'DG.N', 'GRMN.N', 'APTV.N', 'ABBV.N', 'REGN.OQ', 'GM.N', 'ZTS.N', 'NWSA.OQ', 'DAL.N', 'AME.N', 'VRTX.OQ', 'ALLE.N', 'META.OQ', 'MHK.N', 'TSCO.OQ', 'ESS.N', 'GOOG.OQ', 'AVGO.OQ', 'MLM.N', 'URI.N', 'UHS.N', 'RCL.N', 'HCA.N', 'SWKS.OQ', 'HSIC.OQ', 'AAL.OQ', 'EQIX.OQ', 'O.N', 'QRVO.OQ', 'JBHT.OQ', 'WRK.N', 'KHC.OQ', 'AAP.N', 'PYPL.OQ', 'ATVI.OQ', 'UAL.OQ', 'NWS.OQ', 'VRSK.OQ', 'HPE.N', 'SYF.N', 'ILMN.OQ', 'CHD.N', 'WTW.OQ', 'EXR.N', 'CFG.N', 'FRT.N', 'AWK.N', 'UDR.N', 'HOLX.OQ', 'CNC.N', 'ULTA.OQ', 'GPN.N', 'ALK.N', 'DLR.N', 'LKQ.OQ', 'AJG.N', 'TDG.N', 'ALB.N', 'LNT.OQ', 'FTV.N', 'MTD.N', 'CHTR.OQ', 'COO.N', 'MAA.N', 'IDXX.OQ', 'INCY.OQ', 'CBOE.Z', 'REG.OQ', 'DISH.OQ', 'SNPS.OQ', 'RJF.N', 'AMD.OQ', 'ARE.N', 'DXC.N', 'IT.N', 'RE.N', 'HLT.N', 'ALGN.OQ', 'ANSS.OQ', 'PKG.N', 'AOS.N', 'RMD.N', 'MGM.N', 'IQV.N', 'SBAC.OQ', 'DD.N', 'CDNS.OQ', 'NCLH.N', 'HII.N', 'TTWO.OQ', 'MSCI.N', 'EVRG.OQ', 'BR.N', 'FLT.N', 'CPRT.OQ', 'ANET.N', 'ROL.N', 'FTNT.OQ', 'KEYS.N', 'JKHY.OQ', 'LW.N', 'FANG.OQ', 'CE.N', 'FRC.N', 'TFX.N', 'ATO.N', 'WAB.N', 'FOXA.OQ', 'FOX.OQ', 'DOW.N', 'CTVA.N', 'AMCR.N', 'MKTX.OQ', 'TMUS.OQ', 'LDOS.N', 'IEX.N', 'CDW.OQ', 'NVR.N', 'LVS.N', 'NOW.N', 'WRB.N', 'ODFL.OQ', 'LYV.N', 'STE.N', 'ZBRA.OQ', 'PAYC.N', 'IR.N', 'CARR.N', 'OTIS.N', 'DPZ.N', 'DXCM.OQ', 'WST.N', 'BIO.N', 'TDY.N', 'TYL.N', 'TER.OQ', 'CTLT.N', 'ETSY.OQ', 'POOL.OQ', 'TSLA.OQ', 'ENPH.OQ', 'TRMB.OQ', 'MPWR.OQ', 'GNRC.N', 'NXPI.OQ', 'CZR.OQ', 'PTC.OQ', 'CRL.N', 'OGN.N', 'MRNA.OQ', 'TECH.OQ', 'MTCH.OQ', 'BRO.N', 'CDAY.N', 'EPAM.N', 'FDS.N', 'SEDG.OQ', 'CEG.OQ', 'NDSN.OQ', 'MOH.N', 'CPT.N', 'VICI.N', 'ON.OQ', 'KDP.OQ', 'INVH.N', 'CSGP.OQ', 'PCG.N', 'EQT.N', 'TRGP.N', 'ACGL.OQ', 'FSLR.OQ', 'STLD.OQ', 'GEHC.OQ', 'BG.N', 'PODD.OQ', 'FICO.N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugenemesgar/fintech/stockdata/utils/fidatabase.py:742: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  if index > index_date:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rics_of_interest = fi.getSPYList(rd, pd.Timestamp.today().date())\n",
    "\n",
    "\n",
    "print(rics_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_from_rdp = False\n",
    "\n",
    "# Set options for workbook\n",
    "series_name = \"PriceIntrp\"\n",
    "#rdp_series_name = \"TR.PriceClose\"\n",
    "#rdp_series_param = \"Curn=USD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data to pull\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame()\n",
    "series_metadata = {}\n",
    "\n",
    "\n",
    "importlib.reload(fi)\n",
    "\n",
    "if(create_new_from_rdp):\n",
    "    end_date = pd.Timestamp.today().date()\n",
    "    start_date = pd.Timestamp(year=end_date.year-30,month=1,day=1).date()\n",
    "\n",
    "    output_df = fi.ReadTimeSeriesFromRDP(rd,rdp_series_name,rdp_series_param, start_date, end_date,rics_of_interest)\n",
    "    output_df = fi.ClearDupes(output_df)\n",
    "\n",
    "    \n",
    "    series_metadata[\"series_name\"] = series_name\n",
    "    series_metadata[\"rdp_series_name\"] = rdp_series_name\n",
    "    series_metadata[\"rdp_series_param\"] = rdp_series_param\n",
    "    series_metadata[\"base_series\"] = None\n",
    "    series_metadata[\"update_function\"] = None\n",
    "    \n",
    "\n",
    "    series_metadata[\"start_date\"] = output_df.first_valid_index().date()\n",
    "    series_metadata[\"end_date\"] = output_df.last_valid_index().date()\n",
    "    series_metadata[\"ric_list\"] = rics_of_interest\n",
    "\n",
    "    #fi.WriteSeriesToDB(db, series_metadata, output_df,True)\n",
    "else:\n",
    "    result = fi.UpdateSeries(rd,db, series_name)\n",
    "    series_metadata = fi.GetSeriesMetadata(db, series_name)\n",
    "    output_df = fi.ReadSeriesFromDB( db, series_name )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the nan sequnces\n",
    "sorted_key, run_hash= fi.FindNanRuns( output_df )\n",
    "\n",
    "\n",
    "i=0\n",
    "for val in enumerate(sorted_key):\n",
    "    i+=1\n",
    "    \n",
    "    run_count = val[1][1]\n",
    "    run_ric = val[1][0]\n",
    "    if(run_count > 0):\n",
    "        run_list = run_hash[run_ric]\n",
    "        print(  \"RIC: \", run_ric, \" Missing: \", run_count)\n",
    "        for j in range(0, min(5,len(run_list))):\n",
    "            print(f\"    {run_list[j][2]}: from {run_list[j][0]} to {run_list[j][1]}\")\n",
    "\n",
    "        if(i>30):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interoplate the data out\n",
    "output_df.interpolate(method='time', inplace=True,limit_direction='both', limit_area='inside')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_key, run_hash= fi.FindNanRuns( output_df )\n",
    "\n",
    "if( sorted_key[0][1] > 0 ):\n",
    "    print(\"Error: There are still missing values in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the interpolated data to the database\n",
    "importlib.reload(fi)\n",
    "\n",
    "new_metadata = series_metadata.copy()\n",
    "\n",
    "new_metadata[\"series_name\"] = series_metadata[\"series_name\"] + \"Norm\"\n",
    "new_metadata[\"base_series\"] =  series_metadata[\"series_name\"]\n",
    "new_metadata[\"rdp_series_name\"] = None\n",
    "new_metadata['rdp_series_param'] = None\n",
    "new_metadata[\"update_function\"] = \"TransformSeriesToRangeCallback\"\n",
    "\n",
    "fi.UpdateSeriesMetadata(db, new_metadata[\"series_name\"], new_metadata,True)\n",
    "\n",
    "new_df = fi.ReadSeriesFromDB( db, new_metadata[\"series_name\"] )\n",
    "\n",
    "# result = fi.WriteSeriesToDB(db, new_metadata, output_df,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fi)\n",
    "\n",
    "new_df = fi.ReadSeriesFromDB( db, \"PriceIntrpNorm\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"MSFT.OQ\"].plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fi)\n",
    "\n",
    "#\n",
    "# Truncate data series so we can update\n",
    "#\n",
    "\n",
    "if(~create_new_from_rdp):\n",
    "\n",
    "    cds = db.cursor()\n",
    "    \n",
    "    # mdf = fi.GetSeriesMetadata(db, \"Price\")\n",
    "    # mdf[\"end_date\"] = pd.Timestamp(year=2023,month=3,day=14).date()\n",
    "\n",
    "    # sql = f\"DELETE FROM ts_Price WHERE ddate > '{mdf['end_date']}';\"\n",
    "    # cds.execute(sql)\n",
    "    \n",
    "    # result =fi.UpdateSeriesMetadata(db, \"Price\", mdf)\n",
    "\n",
    "\n",
    "    # Update the derived series\n",
    "    #mdf = fi.GetSeriesMetadata(db, \"PriceIntrp\")\n",
    "    #mdf[\"end_date\"] = pd.Timestamp(year=2015,month=3,day=14).date()\n",
    "\n",
    "    #sql = f\"DELETE FROM ts_PriceIntrp WHERE ddate > '{mdf['end_date']}';\"\n",
    "    #cds.execute(sql)\n",
    "\n",
    "    #result = fi.UpdateSeriesMetadata(db, \"PriceIntrp\", mdf)\n",
    "\n",
    "    #db.commit()\n",
    "\n",
    "\n",
    "\n",
    "    #cds.close()\n",
    "\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fi)\n",
    "fi.UpdateSeries( rd, db, \"PriceIntrp\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to database\n",
    "# def WriteSeriesToDB( db, series_metadata, new_df, new_series=False):\n",
    "\n",
    "#     if(new_series==False):\n",
    "#         mdf= fi.GetSeriesMetadata( db, series_metadata[\"series_name\"] )\n",
    "#         if( len(mdf) == 0) :\n",
    "#             print(\"Series does not exist in database.  Please rerun w/ new_series=True.\")\n",
    "#             return False\n",
    "        \n",
    "#     series_name = series_metadata[\"series_name\"]\n",
    "#     rdp_series_name = series_metadata[\"rdp_series_name\"]\n",
    "#     rdp_series_params = series_metadata[\"rdp_series_param\"]\n",
    "#     start_date = series_metadata[\"start_date\"]\n",
    "#     end_date = series_metadata[\"end_date\"]\n",
    "#     ric_list = \",\".join(series_metadata[\"ric_list\"])\n",
    "    \n",
    "#     #rdp_series_name = \"TR.PriceClose\"\n",
    "#     #rdp_series_params = \"Curn=USD\"\n",
    "#     #start_date = new_df.first_valid_index().date()\n",
    "#     #end_date = new_df.last_valid_index().date()\n",
    "#     #ric_list = \",\".join(rics_of_interest)\n",
    "\n",
    "#     table_name=\"\"\n",
    "\n",
    "#     if(new_series==True):\n",
    "#         table_name = fiutil.CreateNewSeriesTable( db, rics_of_interest, series_name )\n",
    "#     else:\n",
    "#         table_name = \"ts_\" + series_name\n",
    "\n",
    "\n",
    "#     dbs = db.cursor()\n",
    "\n",
    "#     header_sql = f\"INSERT INTO {table_name}  ( ddate, \"\n",
    "#     for ric in  new_df.columns:\n",
    "#         col = ric.replace(\".\", \"_\") \n",
    "#         header_sql = header_sql + col + \", \"\n",
    "\n",
    "#     header_sql = header_sql[:-2] \n",
    "\n",
    "#     row_count = len(new_df.index)\n",
    "\n",
    "#     chunk_size = 90\n",
    "\n",
    "#     for i in range(0, row_count, chunk_size):\n",
    "#         sql = header_sql + \") VALUES \\n\"\n",
    "\n",
    "#         next_chunk = min( row_count, chunk_size+i)\n",
    "\n",
    "#         for j in range(i, next_chunk):\n",
    "#             sql = sql + \"('%s', \" % (new_df.index[j].date())\n",
    "#             for ric in  new_df.columns:\n",
    "#                 val = new_df.at[new_df.index[j], ric]\n",
    "#                 if( pd.isna(val) or math.isnan(val) ):\n",
    "#                     sql = sql + \"NULL, \"\n",
    "#                 else:\n",
    "#                     sql = sql + f\"{val:.2f}, \"\n",
    "\n",
    "#             sql = sql[:-2] \n",
    "#             sql = sql + \"),\\n\"\n",
    "            \n",
    "\n",
    "#         sql = sql[:-2] \n",
    "#         sql = sql + \";\"\n",
    "        \n",
    "#         #print(sql)\n",
    "#         dbs.execute(sql)\n",
    "\n",
    "#     meta_sql=\"\"\n",
    "#     if(new_series==True):\n",
    "#         meta_sql = f\"INSERT INTO TimeSeries_MetaData ( series_name, rdp_series_name, rdp_series_parms, start_date, end_date, ric_list ) VALUES \"\n",
    "#         meta_sql = meta_sql + f\"('{series_name}', '{rdp_series_name}', '{rdp_series_params}', '{start_date}', '{end_date}', '{ric_list}');\"\n",
    "#     else:\n",
    "#         meta_sql = f\"UPDATE TimeSeries_MetaData SET end_date='{end_date}' WHERE series_name='{series_name}';\"\n",
    "    \n",
    "\n",
    "#     dbs.execute(meta_sql)\n",
    "#     db.commit()\n",
    "#     dbs.close()\n",
    "#     return True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file \"S&P500PriceHistory.csv\n",
    "# The file includes a header row\n",
    "# Intrepret dates as dates\n",
    "# The date column is the index\n",
    "\n",
    "# if(pull_from_refinitiv==False):\n",
    "#     output_df = pd.read_csv(\"S&P500PriceHistory.csv\", parse_dates=['Date'], index_col='Date')\n",
    "# else:\n",
    "#     stamp = str(datetime.now())\n",
    "#     output_file = f\"S&P500PriceHistory-{start_date}-{end_date}-{stamp}.csv\"\n",
    "#     output_df.to_csv(output_file)\n",
    "#     print(\"Wrote to\", output_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamp = str(datetime.now())\n",
    "output_file = f\"CLEANED S&P500PriceHistory-{start_date}-{end_date}-{stamp}.csv\"\n",
    "new_df.to_csv(output_file)\n",
    "print(\"Wrote to\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot new_df[\"MRO.N\"] from 1990 to 1995\n",
    "\n",
    "#interp_d2[\"CSCO.OQ\"].plot(x=\"Date\", ylim=[0,1.5], xlim=[pd.Timestamp(year=1990,day=1,month=1).date(),pd.Timestamp(year=1993,day=1,month=1).date()], y=\"MRO.N\", figsize=(20,10), title=\"MRO.N\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_df = new_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#interp_d2 = interp_df.applymap( lambda x: np.nan if pd.isna(x) else float(x) )\n",
    "                            \n",
    "interp_df.interpolate(method='time', inplace=True,limit_direction='both', limit_area='inside')\n",
    "\n",
    "new_df_old = new_df.copy()\n",
    "new_df = interp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_df = new_df_old\n",
    "#interp_d2[\"CSCO.OQ\"].plot(x=\"Date\", ylim=[0,1.5], xlim=[pd.Timestamp(year=1991,day=1,month=1).date(),pd.Timestamp(year=1991,day=1,month=1).date()], y=\"MRO.N\", figsize=(20,10), title=\"MRO.N\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented Out -- Former code to write to database\n",
    "\n",
    "\n",
    "# index = 0\n",
    "# col_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# if(pull_from_refinitiv==True):\n",
    "\n",
    "#     dbs = db.cursor()\n",
    "#     db.begin()\n",
    "\n",
    "#     for col in output_df.columns:\n",
    "#         print(f\"Processing column {col} ({col_count} of {len(output_df.columns)})\")\n",
    "\n",
    "\n",
    "#         col_count = col_count + 1\n",
    "#         sql = \"\"\n",
    "#         to_write = output_df[col].copy()\n",
    "#         to_write.dropna(inplace=True)\n",
    "\n",
    "#         first_col = 0\n",
    "#         last_col = len(to_write)\n",
    "        \n",
    "        \n",
    "#         chunk_size = 10000\n",
    "        \n",
    "        \n",
    "#         for i in range(first_col, last_col, chunk_size):\n",
    "#             sql = \"INSERT INTO TimeSeries ( ric, series, variant, ddate, val ) VALUES \" \n",
    "    \n",
    "\n",
    "#             next_chunk = min( last_col, chunk_size+i)\n",
    "#             for j in range(i, next_chunk):\n",
    "\n",
    "#                 date_val = str(to_write.index[j].date())\n",
    "#                 value_val = to_write[j]\n",
    "#                 #my_data.append( ( col, \"Price\", \"RDP\", date_val, value_val) ) \n",
    "\n",
    "#                 sql = sql + f'(\"{col}\", \"Price\", \"RDP\", \"{date_val}\", \"{value_val:.2f}\")'\n",
    "#                 if( j < (next_chunk-1) ):\n",
    "#                     sql = sql + \",\"\n",
    "#                 else:\n",
    "#                     sql = sql + \";\"\n",
    "\n",
    "#                 sql = sql + \"\\n\"\n",
    "        \n",
    "        \n",
    "#             ## Execute the sql statement stored in variable sql\n",
    "#             dbs.execute(sql)\n",
    "            \n",
    "#     db.commit()\n",
    "#     dbs.close()\n",
    "\n",
    "    \n",
    "       \n",
    "       \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write to database\n",
    "# def WriteSeriesToDB( db, series_metadata, new_df, new_series=False):\n",
    "\n",
    "#     if(new_series==False):\n",
    "#         mdf= fiutil.GetSeriesMetadata( db, series_metadata[\"series_name\"] )\n",
    "#         if( len(mdf) == 0) :\n",
    "#             print(\"Series does not exist in database.  Please rerun w/ new_series=True.\")\n",
    "#             return False\n",
    "        \n",
    "#     series_name = series_metadata[\"series_name\"]\n",
    "#     rdp_series_name = series_metadata[\"rdp_series_name\"]\n",
    "#     rdp_series_params = series_metadata[\"rdp_series_params\"]\n",
    "#     start_date = series_metadata[\"start_date\"]\n",
    "#     end_date = series_metadata[\"end_date\"]\n",
    "#     ric_list = \",\".join(series_metadata[\"ric_list\"])\n",
    "    \n",
    "#     #rdp_series_name = \"TR.PriceClose\"\n",
    "#     #rdp_series_params = \"Curn=USD\"\n",
    "#     #start_date = new_df.first_valid_index().date()\n",
    "#     #end_date = new_df.last_valid_index().date()\n",
    "#     #ric_list = \",\".join(rics_of_interest)\n",
    "\n",
    "#     table_name=\"\"\n",
    "\n",
    "#     if(new_series==True):\n",
    "#         table_name = fiutil.CreateNewSeriesTable( db, rics_of_interest, series_name )\n",
    "#     else:\n",
    "#         table_name = \"ts_\" + series_name\n",
    "\n",
    "\n",
    "#     dbs = db.cursor()\n",
    "\n",
    "#     header_sql = f\"INSERT INTO {table_name}  ( ddate, \"\n",
    "#     for ric in  new_df.columns:\n",
    "#         col = ric.replace(\".\", \"_\") \n",
    "#         header_sql = header_sql + col + \", \"\n",
    "\n",
    "#     header_sql = header_sql[:-2] \n",
    "\n",
    "#     row_count = len(new_df.index)\n",
    "\n",
    "#     chunk_size = 90\n",
    "\n",
    "#     for i in range(0, row_count, chunk_size):\n",
    "#         sql = header_sql + \") VALUES \\n\"\n",
    "\n",
    "#         next_chunk = min( row_count, chunk_size+i)\n",
    "\n",
    "#         for j in range(i, next_chunk):\n",
    "#             sql = sql + \"('%s', \" % (new_df.index[j].date())\n",
    "#             for ric in  new_df.columns:\n",
    "#                 val = new_df.at[new_df.index[j], ric]\n",
    "#                 if( pd.isna(val) or math.isnan(val) ):\n",
    "#                     sql = sql + \"NULL, \"\n",
    "#                 else:\n",
    "#                     sql = sql + f\"{val:.2f}, \"\n",
    "\n",
    "#             sql = sql[:-2] \n",
    "#             sql = sql + \"),\\n\"\n",
    "            \n",
    "\n",
    "#         sql = sql[:-2] \n",
    "#         sql = sql + \";\"\n",
    "        \n",
    "#         #print(sql)\n",
    "#         dbs.execute(sql)\n",
    "\n",
    "#     meta_sql=\"\"\n",
    "#     if(new_series==True):\n",
    "#         meta_sql = f\"INSERT INTO TimeSeries_MetaData ( series_name, rdp_series_name, rdp_series_parms, start_date, end_date, ric_list ) VALUES \"\n",
    "#         meta_sql = meta_sql + f\"('{series_name}', '{rdp_series_name}', '{rdp_series_params}', '{start_date}', '{end_date}', '{ric_list}');\"\n",
    "#     else:\n",
    "#         meta_sql = f\"UPDATE TimeSeries_MetaData SET end_date='{end_date}' WHERE series_name='{series_name}';\"\n",
    "    \n",
    "\n",
    "#     dbs.execute(meta_sql)\n",
    "#     db.commit()\n",
    "#     dbs.close()\n",
    "#     return True\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented Out -- Former code to read from database\n",
    "#\n",
    "\n",
    "# start_date=pd.Timestamp(year=2000,day=1,month=1)\n",
    "# end_date=pd.Timestamp(year=2023,day=1,month=1)\n",
    "\n",
    "\n",
    "# #Find the valid days in the calendar that there is data for\n",
    "# valid_days = mcal.get_calendar('NYSE').schedule(start_date=start_date, end_date=end_date).index.to_list()\n",
    "\n",
    "# # Create an empty dayaframe with the index & columns we want\n",
    "# read_df = pd.DataFrame(np.nan, columns=rics_of_interest,index=valid_days)\n",
    "\n",
    "# #Create the SQL statement to query the DB\n",
    "# sql = \"SELECT ric, ddate, val FROM TimeSeries WHERE variant = 'RDP' AND ddate >= '%s' AND ddate <= '%s'\" % (start_date, end_date)\n",
    "\n",
    "\n",
    "# #Read the SQL into a dataframe\n",
    "# ts = time.perf_counter()\n",
    "# raw_df = pd.read_sql(sql, db, parse_dates=True, coerce_float=True)\n",
    "# te = time.perf_counter()\n",
    "\n",
    "# print(f\"Read {len(raw_df)} rows in {te-ts:0.4f} seconds\")\n",
    "\n",
    "# ts = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "# for row in raw_df.itertuples():\n",
    "#     read_df.at[ pd.Timestamp(row.ddate), row.ric ] = row.val\n",
    "\n",
    "\n",
    "# te = time.perf_counter()\n",
    "# print(f\"Processed SQL to DF rows in {te-ts:0.4f} seconds\")\n",
    "\n",
    "# read_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
